{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fe9368-c00c-43ca-9eef-9e387fa79f86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33004156-577a-4a4f-9b58-d19af6d63ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.unmount('/mnt/data')\n",
    "print('unmount completed')\n",
    "print('begin re-mount')\n",
    "storageAccountName = \"blobcontainershilsdemo\"\n",
    "\n",
    "# Make sure your SAS token is not expired by validating it in you Azure portal>> Storage Account >> Shared Access Signature\n",
    "# Generate a new token just to be sure that the code doesn't throw any authentication error\n",
    "sasToken = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-04-02T02:50:52Z&st=2024-04-01T18:50:52Z&spr=https&sig=DQHW9q022OBc8M2r1%2Bu4V8ubBagwAv%2FqMdeSojMSyxI%3D\"\n",
    "blobContainerName = \"democontainer1\"\n",
    "mountPoint = \"/mnt/data/\"\n",
    "print(mountPoint)\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "    source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n",
    "    mount_point = mountPoint,\n",
    "    #extra_configs = {'fs.azure.account.key.' + storageAccountName + '.blob.core.windows.net': storageAccountAccessKey}\n",
    "    extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n",
    ")\n",
    "    print(\"mount succeeded!\")\n",
    "except Exception as e:\n",
    "    print(\"mount exception\", e)\n",
    "print('remount completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883e63f8-1b28-4694-a6f3-b73b6639cdb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lists all the files in NYSE and NASDAQ in your Azure blob\n",
    "dbutils.fs.ls(\"/mnt/data/NYSE\")\n",
    "dbutils.fs.ls(\"/mnt/data/NASDAQ\")\n",
    "dbutils.fs.ls(\"/mnt/data/NASDAQ1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f5a937-ea01-43f1-8888-6968723d87be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.master('local').appName('guided-capstone').getOrCreate()\n",
    "raw = spark.read.format(\"csv\").load(\"dbfs:/mnt/data/NYSE/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a334bba-4a8c-4f39-82df-0212120d86fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from typing import List\n",
    "from pyspark.sql.types import StructType, StructField, DateType, StringType, TimestampType, IntegerType, DecimalType\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the schema for the common event\n",
    "common_event = StructType([\n",
    "    StructField(\"trade_dt\", DateType(), True),\n",
    "    # StructField(\"event_tm\", TimestampType(), True),\n",
    "    StructField(\"event_tm\", StringType(), True),\n",
    "    StructField(\"rec_type\", StringType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    # StructField(\"arrival_tm\", TimestampType(), True),\n",
    "    StructField(\"arrival_tm\", StringType(), True),\n",
    "    StructField(\"event_seq_nb\", IntegerType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"trade_pr\", DecimalType(10, 2), True),\n",
    "    StructField(\"trade_size\", IntegerType(), True),\n",
    "    StructField(\"bid_pr\", DecimalType(10, 2), True),\n",
    "    StructField(\"bid_size\", IntegerType(), True),\n",
    "    StructField(\"ask_pr\", DecimalType(10, 2), True),\n",
    "    StructField(\"ask_size\", IntegerType(), True),\n",
    "    StructField(\"ext_rec_type\", StringType(), True),\n",
    "    StructField(\"flag\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Print the schema\n",
    "print(common_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed69226-ac70-41f1-a7a7-0ae358b6e030",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "\n",
    "def parse_csv(row):\n",
    "    record = [row._c0,row._c1,row._c2,row._c3,row._c4,row._c5,row._c6,row._c7,row._c8,row._c9,row._c10]\n",
    "    try:\n",
    "        trade_dt = datetime.strptime(record[0], \"%Y-%m-%d\") if record[0] else None\n",
    "        # event_tm = datetime.strptime(record[1], \"%Y-%m-%d %H:%M:%S\") if record[1] else None\n",
    "        event_tm = record[1]\n",
    "        rec_type = record[2]\n",
    "        symbol = record[3]\n",
    "        # arrival_tm = datetime.strptime(record[4], \"%Y-%m-%d %H:%M:%S\") if record[4] else None\n",
    "        arrival_tm = record[4]\n",
    "        event_seq_nb = int(record[5]) if record[5] else None\n",
    "        exchange = record[6]\n",
    "        #trade_pr = Decimal(record[7]) if record[7] else None\n",
    "        if rec_type == \"T\":\n",
    "            # Event for record type \"T\"\n",
    "            trade_pr = Decimal(record[7]) if record[7] else None\n",
    "            trade_size = int(record[8]) if record[8] else None\n",
    "            event = [trade_dt, event_tm, rec_type, symbol, arrival_tm, event_seq_nb, exchange,trade_pr, trade_size, None, None, None, None, \"T\", \"\"]\n",
    "        elif rec_type == \"Q\":\n",
    "            # Event for record type \"Q\"\n",
    "            bid_pr = Decimal(record[7]) if record[7] else None\n",
    "            bid_size = int(record[8]) if record[8] else None\n",
    "            ask_pr = Decimal(record[9]) if record[9] else None\n",
    "            ask_size = int(record[10]) if record[10] else None\n",
    "            event = [trade_dt, event_tm, rec_type, symbol, arrival_tm, event_seq_nb, exchange, None, None, bid_pr, bid_size, ask_pr, ask_size, \"Q\", \"\"]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid record type\")\n",
    "        \n",
    "        return event\n",
    "    except Exception as e:\n",
    "        # If there's any error, return a common event with \"B\" as record type\n",
    "        return [None, None, None, None, None, None, None, None, None, None, None, None, None,\"B\", \"\"]\n",
    "\n",
    "parsed = raw.rdd.map(parse_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d44998-5f14-430b-9c12-7f4797a20d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(parsed, common_event)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "233fdad7-6df1-40da-ac7f-48a8a57bddb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/mnt/data/output_csv\")\n",
    "output_dir = \"/mnt/data/output_csv\"\n",
    "df.write.partitionBy(\"rec_type\").mode(\"overwrite\").parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "256916f3-2bdd-4752-9b3b-0d201f887f46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for row in parsed.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcf49b5-563e-4a78-9eca-c9ac10816e03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"guided-capstone\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.json(\"dbfs:/mnt/data/NASDAQ/*.txt\")\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b58622-ebe9-48a1-9fa7-b3816774521a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"guided-capstone\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.json(\"dbfs:/mnt/data/NASDAQ/*.txt\")\n",
    "df.show()\n",
    "def parse_json(row):\n",
    "    record = json.loads(row)  # Parse JSON string into a dictionary\n",
    "    try: \n",
    "        if df.event_type == \"T\":\n",
    "            df.write.partitionBy(\"event_type\").mode(\"overwrite\").parquet(\"output_dir\")\n",
    "        elif df.event_type == \"Q\":\n",
    "            # Event for record type \"Q\"\n",
    "            df.write.partitionBy(\"event_type\").mode(\"overwrite\").parquet(\"output_dir\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid record type\")\n",
    "        return row\n",
    "    except Exception as e:\n",
    "        # Handle exceptions: return dummy event in bad partition and fill in the fields as None or empty string\n",
    "        return [row, \"B\"]\n",
    "\n",
    "parsed = raw.rdd.map(parse_json)\n",
    "# data = spark.createDataFrame(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1559692c-9d3e-45bd-a05d-29b4ab44115d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/mnt/data/output_json\")\n",
    "#databricks fs mkdirs dbfs:/mnt/data/output\n",
    "output_dir = \"/mnt/data/output_json\"\n",
    "\n",
    "# Write the DataFrame to Parquet format partitioned by \"event_type\"\n",
    "df.write.partitionBy(\"event_type\").mode(\"overwrite\").parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d01691-2529-480d-8891-0d271e7e4882",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Code for Guided Capstone Step 3 starts here ###\n",
    "### End-of-Day (EOD) Data Load ###\n",
    "\n",
    "trade_common_json = spark.read.parquet(\"/mnt/data/output_json/event_type=T\")\n",
    "trade_common_csv = spark.read.parquet(\"/mnt/data/output_csv/rec_type=T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "878371d0-853d-4fa0-8ac8-a7e35d946dde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#selecting those fields that belong to trade data and removing unnecessary fields\n",
    "trade1 = trade_common_csv.select(\"trade_dt\",\"event_tm\", \"symbol\",\"arrival_tm\",\"event_seq_nb\",\"exchange\",\"trade_pr\",\"trade_size\",\"ext_rec_type\")\n",
    "\n",
    "trade2 = trade_common_json.select(\"event_tm\",\"event_seq_nb\",\"exchange\",\"execution_id\",\"file_tm\",\"price\",\"size\",\"symbol\",\"trade_dt\")\n",
    "\n",
    "trade_json_with_col_renamed= trade2.withColumnRenamed(trade2.columns[4],\"arrival_tm\")\n",
    "trade_json_with_col_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1692af0-b0f3-443e-ad78-cc924ee87757",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "#data correction function\n",
    "def applyLatest(df):\n",
    "    # unique col fields that identify the trade record\n",
    "    unique_combo = [\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\", \"event_seq_nb\"]\n",
    "    #GroupBy by the unique_combo key and aggregate to get the row with the latest arrival_tm\n",
    "    latestRows = df.groupBy(*unique_combo).agg({\"arrival_tm\": \"max\"}).alias(\"latest_arrival_tm\")\n",
    "    # Joining the original dataframe(df) and the latestRows dataframe. Syntax: dataframe.join(dataframe1, \n",
    "               #dataframe.ID == dataframe1.ID, \n",
    "               #\"inner\").show() \n",
    "    latest_output_rows = latestRows.join(df, (latestRows.trade_dt == df.trade_dt) &\n",
    "                                      (latestRows.symbol == df.symbol) &\n",
    "                                      (latestRows.exchange == df.exchange) &\n",
    "                                      (latestRows.event_tm == df.event_tm) &\n",
    "                                      (latestRows.event_seq_nb == df.event_seq_nb),\"inner\"\n",
    "                                )\n",
    "    return latestRows\n",
    "# Calling applyLatest() function pasing trade1 DataFrame as the parameter\n",
    "TradeCorrected_csv = applyLatest(trade1)\n",
    "TradeCorrected_json = applyLatest(trade_json_with_col_renamed)\n",
    "TradeCorrected_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8465610e-825a-4f4f-bf41-0eb87e1e5b35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trade_date = \"2020-08-05\"\n",
    "output_path = \"/mnt/data/EOD_trade_data\"\n",
    "\n",
    "df_write = TradeCorrected_json.filter(TradeCorrected_json.trade_dt == trade_date)\n",
    "#df_write.write.parquet(\"{}/trade_dt={}\".format(output_path, trade_date))\n",
    "df_write.write.mode(\"append\").parquet(\"{}/trade_dt={}\".format(output_path, trade_date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b18dc1d-2733-48e0-81ef-377b93695764",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "quote_common_json = spark.read.parquet(\"/mnt/data/output_json/event_type=Q\")\n",
    "quote_common_csv = spark.read.parquet(\"/mnt/data/output_csv/rec_type=Q\")\n",
    "#quote_common_json.show()\n",
    "#quote_common_csv.show()\n",
    "\n",
    "#quote_common_csv field format: |  trade_dt|event_tm|symbol|arrival_tm|event_seq_nb|exchange|trade_pr|trade_size|bid_pr|bid_size|ask_pr|ask_size|ext_rec_type|flag\n",
    "# quote_common_json field format:| ask_pr|ask_size| bid_pr|bid_size|event_seq_nb|event_tm|exchange|execution_id|file_tm|price|size|symbol|trade_dt|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5790a933-1ea1-46a4-af0f-c373c9fa0d33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#selecting those fields that belong to trade data and removing unnecessary fields\n",
    "Quote1 = quote_common_csv.select(\"trade_dt\",\"event_tm\", \"symbol\", \"arrival_tm\", \"event_seq_nb\", \"exchange\", \"bid_pr\", \"bid_size\", \"ask_pr\", \"ask_size\", \"ext_rec_type\")\n",
    "\n",
    "Quote2 = quote_common_json.select(\"ask_pr\",\"ask_size\",\"bid_pr\",\"bid_size\",\"event_seq_nb\",\"event_tm\", \"exchange\", \"execution_id\", \"file_tm\", \"price\",\"size\",\"symbol\",\"trade_dt\")\n",
    "\n",
    "quote_json_with_col_renamed= Quote2.withColumnRenamed(Quote2.columns[8],\"arrival_tm\")\n",
    "quote_json_with_col_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd196272-d8f4-4e8a-86cc-e20648cd5ee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "#data correction function\n",
    "def applyLatest(df):\n",
    "    # unique col fields that identify the quote record\n",
    "    unique_combo = [\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\", \"event_seq_nb\"]\n",
    "    #GroupBy by the unique_combo key and aggregate to get the row with the latest arrival_tm\n",
    "    latestRows = df.groupBy(*unique_combo).agg({\"arrival_tm\": \"max\"}).alias(\"latest_arrival_tm\")\n",
    "    # Joining the original dataframe(df) and the latestRows dataframe. Syntax: dataframe.join(dataframe1, \n",
    "               #dataframe.ID == dataframe1.ID, \n",
    "               #\"inner\").show() \n",
    "    latest_output_rows = latestRows.join(df, (latestRows.trade_dt == df.trade_dt) &\n",
    "                                      (latestRows.symbol == df.symbol) &\n",
    "                                      (latestRows.exchange == df.exchange) &\n",
    "                                      (latestRows.event_tm == df.event_tm) &\n",
    "                                      (latestRows.event_seq_nb == df.event_seq_nb),\"inner\"\n",
    "                                )\n",
    "    return latestRows\n",
    "# Calling applyLatest() function pasing trade1 DataFrame as the parameter\n",
    "QuoteCorrected_csv = applyLatest(Quote1)\n",
    "QuoteCorrected_json = applyLatest(quote_json_with_col_renamed)\n",
    "QuoteCorrected_json.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ad0191-806f-47b5-8b66-1ed37f7ef6bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trade_date = \"2020-08-05\"\n",
    "output_path = \"/mnt/data/EOD_quote_data\"\n",
    "\n",
    "# Note: First write the CSV file, then ensure to change the \"QuoteCorrected_csv\" to \"QuoteCorrected_json\" to append JSON files to EOD_quote_data folder on Azure Blob storage. \n",
    "\n",
    "df_write = QuoteCorrected_csv.filter(QuoteCorrected_csv.trade_dt == trade_date)\n",
    "#df_write.write.parquet(\"{}/trade_dt={}\".format(output_path, trade_date))\n",
    "df_write.write.mode(\"append\").parquet(\"{}/trade_dt={}\".format(output_path, trade_date))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Guided_capstone_step2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
